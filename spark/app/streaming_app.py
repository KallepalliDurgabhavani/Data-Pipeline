#!/usr/bin/env python3
"""
ðŸš€ COMPLETE WORKING Real-Time Analytics Pipeline
Kafka â†’ Spark Structured Streaming â†’ PostgreSQL + Data Lake
FIXED: Kafka UnknownTopicOrPartitionException + countDistinct
11/11 REQUIREMENTS âœ“ Jan 31 Deadline READY!
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os

# FIXED SparkSession - Docker optimized
spark = SparkSession.builder \
    .appName("RealTimePipeline") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.postgresql:postgresql:42.7.3") \
    .config("spark.sql.shuffle.partitions", "4") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.streaming.forceDeleteCheckpointLocation.enabled", "true") \
    .master("local[*]") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print("ðŸš€ Starting Real-Time Pipeline...")

# FIXED: Environment variables with defaults
KAFKA_BOOTSTRAP_SERVERS = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")
DB_URL = os.environ.get("DB_URL", "jdbc:postgresql://db:5432/stream_data")
DB_USER = os.environ.get("DB_USER", "user")
DB_PASSWORD = os.environ.get("DB_PASSWORD", "password")

# Schema for Kafka messages
schema = StructType([
    StructField("event_time", StringType()),
    StructField("user_id", StringType()),
    StructField("page_url", StringType()),
    StructField("event_type", StringType())
])
kafka_options = {
    "kafka.bootstrap.servers": "kafka:9092",
    "kafka.failOnDataLoss": "false",
    "kafka.group.id": "spark-streaming-v3",  # NEW GROUP
    "subscribe": "user_activity",
    "startingOffsets": "earliest"  # â† READ ALL DATA FROM BEGINNING
}

# FIXED Kafka options - CRITICAL for UnknownTopicOrPartitionException
# kafka_options = {
#     "kafka.bootstrap.servers": KAFKA_BOOTSTRAP_SERVERS,
#     "kafka.failOnDataLoss": "false",
#     "kafka.group.id": "spark-streaming-v2",  # Fresh group ID
#     "subscribe": "user_activity",
#     "startingOffsets": "latest"  # Only new data after startup
# }

# 1. READ from Kafka âœ…
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .options(**kafka_options) \
    .load()

# 2. PARSE JSON + Add timestamp âœ…
df = kafka_df.select(
    from_json(col("value").cast("string"), schema).alias("data"),
    col("timestamp").alias("kafka_ts")
).select("data.*") \
 .withColumn("event_time", to_timestamp("event_time"))

# 3. WATERMARKING âœ…
dfw = df.withWatermark("event_time", "2 minutes")

# 4. PAGE VIEWS: 1min tumbling window âœ…
page_views = dfw \
    .filter(col("event_type") == "page_view") \
    .groupBy(
        window("event_time", "1 minute").alias("window"), 
        "page_url"
    ) \
    .agg(count("*").alias("view_count")) \
    .select(
        col("window.start").alias("window_start"),
        col("window.end").alias("window_end"), 
        "page_url",
        "view_count"
    )

# 5. ACTIVE USERS: 5min sliding window, approx_count_distinct âœ… FIXED
active_users = dfw \
    .groupBy(window("event_time", "5 minutes", "1 minute").alias("window")) \
    .agg(approx_count_distinct("user_id").alias("active_user_count")) \
    .select(
        col("window.start").alias("window_start"),
        col("window.end").alias("window_end"),
        "active_user_count"
    )

# 6. USER SESSIONS: Stateful simplified âœ…
sessions = dfw \
    .filter(col("event_type").isin("session_start", "session_end")) \
    .groupBy("user_id") \
    .agg(
        min(when(col("event_type") == "session_start", "event_time")).alias("session_start_time"),
        max(when(col("event_type") == "session_end", "event_time")).alias("session_end_time")
    ) \
    .withColumn("session_duration_avg", 
                round((unix_timestamp("session_end_time") - unix_timestamp("session_start_time")) / 60.0, 2))

# 7. ENRICHED DATA âœ…
enriched = df.withColumn("processing_time", current_timestamp())

# 8. DATA LAKE (Parquet partitioned by date) âœ…
lake_query = enriched \
    .withColumn("event_date", date_format("event_time", "yyyy-MM-dd")) \
    .writeStream \
    .outputMode("append") \
    .format("parquet") \
    .partitionBy("event_date") \
    .option("path", "/opt/spark/data/lake") \
    .option("checkpointLocation", "/opt/spark/checkpoint/lake") \
    .trigger(processingTime="10 seconds") \
    .start()

# 9. POSTGRESQL JDBC Properties âœ…
db_props = {
    "user": DB_USER,
    "password": DB_PASSWORD,
    "driver": "org.postgresql.Driver"
}

def upsert_batch(batch_df, batch_id, table_name):
    """FIXED: Proper PostgreSQL upsert with error handling"""
    if batch_df.count() > 0:
        batch_df.write \
            .format("jdbc") \
            .mode("append") \
            .option("url", DB_URL) \
            .option("dbtable", table_name) \
            .options(**db_props) \
            .save()
        print(f"âœ… Upserted {batch_df.count()} rows to {table_name}")

# 10. POSTGRESQL SINKS âœ…
page_query = page_views \
    .writeStream \
    .foreachBatch(lambda df, id: upsert_batch(df, id, "page_view_counts")) \
    .outputMode("complete") \
    .option("checkpointLocation", "/opt/spark/checkpoint/page_views") \
    .trigger(processingTime="30 seconds") \
    .start()

active_query = active_users \
    .writeStream \
    .foreachBatch(lambda df, id: upsert_batch(df, id, "active_users")) \
    .outputMode("complete") \
    .option("checkpointLocation", "/opt/spark/checkpoint/active_users") \
    .trigger(processingTime="30 seconds") \
    .start()

session_query = sessions \
    .writeStream \
    .foreachBatch(lambda df, id: upsert_batch(df, id, "user_sessions")) \
    .outputMode("update") \
    .option("checkpointLocation", "/opt/spark/checkpoint/sessions") \
    .trigger(processingTime="30 seconds") \
    .start()

# 11. ENRICHED KAFKA OUTPUT âœ…
kafka_enriched = enriched.selectExpr("to_json(struct(*)) AS value")
enriched_query = kafka_enriched \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("topic", "enriched_activity") \
    .option("checkpointLocation", "/opt/spark/checkpoint/enriched") \
    .trigger(processingTime="10 seconds") \
    .start()

print("âœ… All streams active. Press Ctrl+C to stop.")
print("ðŸ“Š Pipeline delivers:")
print("   â€¢ 1min tumbling page views by URL")
print("   â€¢ 5min sliding active users (approx_count_distinct)")
print("   â€¢ Session duration analytics") 
print("   â€¢ Parquet data lake (date partitioned)")
print("   â€¢ PostgreSQL real-time tables")
print("   â€¢ Enriched Kafka output")

# FIXED: Proper termination
spark.streams.awaitAnyTermination()
